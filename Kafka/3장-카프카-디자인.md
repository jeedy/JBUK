# 3장 카프카 디자인

메시지 플랫폼

## 3.1 카프카 디자인의 특징

### 3.1.1 분산 시스템

> **사례**<br>
> 초당 1,000개의 메시지를 처리하는(CPU사용률 100%) 물리적인 서버 1대가 있을때, 이 서버의 최대 처리 가능 수준은 1,000/초 입니다.
> 이 서버에 만약 초당 900개의 메시지가 들어올 경우 CPU사용량은 90% 정도이며, 서버에 장애가 발생하면 바로 서비스 중지가 될 수 있는 상황입니다.
>
> 이때, 서버 1대를 추가해 서버로 들어오는 메시지를 분산할 수 있다면, 
> 각 서버는 450개의 메시지를 처리 하므로 CPU사용률도 45% 정도로 낮아지며 매우 안정적으로 운영 할 수 있습니다.
    
- 단일 시스템보다 더 높은 성능 보장
- 분산 시스템 중 하나의 서버 또는 노드 등이 장애가 발생하면 다른 서버 또는 노드가 대신 처리
- 시스템 확장이 용이


### 3.1.2 페이지 캐시

카프카의 처리량을 높이기 위해 OS의 페이지 캐시를 이용하도록 디자인 되었다. 
이런 장점 덕분에 디스크 중에서 가격이 가장 저렴한 SATA 디스크를 사용해도 무방하다.
그러나 주의할 점은 페이지 캐시를 서로 공유해야 하기 때문에 하나의 시스템에 카프카를 다른 애플리케이션과
함께 실행하는 것은 권장하지 않는다. 

> **JVM 힙 사이즈**<br>
> 카프카 기본값은 1G, 설정 변경 방법은 JMX 설정 방법을 참고해 KAFKA_HEAP_OPTS="-Xmx6G -Xms6G"로 추가하면 된다.
> 카프카는 초당 메시지 단위, 메가비트 단위를 처리함에 있어 5GB의 힙 메모리면 충분하고, 남아 있는 메모리는
> 페이지 캐시로 사용하기를 권장


### 3.1.3 배치 전송 처리

카프카에서는 작은 I/O 들을 묶어서 처리할 수 있도록 배치 작업으로 처리

> **예시**<br>
> 메시지 보내는 시간을 1초라고 가정한다면, 작은 메시지 4개를 보내는 데 총 4초가 소요, 이러한 작은 메시지들을
> 묶어서 한 번에 보내게 되면 네트워크 왕복 오버헤드 등을 줄이게 되어 메시지 4개를 보내는 데 1초의 시간이 소요된다.


## 3.2 카프카 데이터 모델

토픽과 파티션 

### 3.2.1 토픽의 이해

카프카는 데이터를 구분하기 위한 단위로 토픽(topic)이라는 용어를 사용한다.
(메일 시스템과 비교하면, 토픽은 **메일주소**) 

토픽 이름은 249자 미만으로 영문, 숫자, '.', '_', '-' 를 조합하여 자유롭게 만들수 있다. 
여러 서비스와 공통으로 사용하기 위해 접두어를 넣는 것을 추천.


### 3.2.2 파티션의 이해

카프카에서 파티션이란 토픽을 분할한 것입니다.

예를 들어 프로듀서(1대)가 카프카 *뉴스토픽*에 메시지를 4개 보낸다고 할때 각 메시지당 1초가 걸린다고 가정하면 총 4초의 시간이 걸린다.
이를 프로듀서 4대가 메시지를 분활해 동시에 메시지를 한개씩 보낸다면 총 4개의 메시지가 카프카에게 전달되는데, 
이때 기대하는 효과는 4배의 성능을 기대하지만 카프카는 그렇지 않다. 이는 **메시지 큐 시스템** 특성상 메시지의 순서가
보장되어야 하기 때문에 4대의 프로듀서가 동시에 메시지를 날리더라도 순서대로 받아서 처리를 해야하기 때문이다.
그래서 카프카에서 효율적인 메시지 전송과 속도를 높이기 위해 토픽의 파티션 개념이 필요하다.

결국, 프로듀서 4대를 이용해서 4배의 효과를 보장하기 위해서는 토픽의 파티션도 4개로 늘려줘야한다. (프로듀서 수=파티션 수)
 
#### 무족건 파티션 수를 늘려야 하나?
- 파일 핸들러의 낭비
- 장애 복구 시간 증가

#### 적절한 파티션 수는?
파티션 선정기준, 먼저 목표 처리량의 기준을 잡는다. 예를 들어 4개의 프로듀서를 통해 각각 초당 10개의 메시지를
보낸다고 하면, 카프카의 토픽에서 초당 40개의 메시지를 받아줘야 한다. 만약 해당 토픽에서 파티션을 1개로 했을 때,
초당 10개의 메시지만 받아준다면 파티션을 4로 늘린다. (프로듀서가 4대이므로 파티션을 4이상 늘릴 필요는 없다. **프로듀서 >= 파티션**)

하지만 카프카에서는 컨슈머도 있기 때문에 컨슈머의 입장도 고려가 필요하다.
8대의 컨슈머가 초당 5개의 메시지를 가져간다면, 해당 토픽의 파티션 수는 컨슈머 수와 동일하게 8개로 맞추어 각각 파티션에 접근
할 수 있게 해야한다.(컨슈머가 8대이므로 파티션을 8이상 늘릴 필요는 없다. **컨슈머 >= 파티션**)

**주의** 
- 파티션 수를 늘리는건 가능하지만 줄이는건 불가능하다.
- 브로커당 약 2,000개 정도의 최대 파티션 수를 권장

이 때문에 적은수의 파티션으로 운영해보고, 프로듀서 또는 컨슈머에서 병목현상이 발생하게 될 때 조금씩 파티션 수와
프로듀서 또는 컨슈머를 늘려가는 방법으로 운영하도록 추천한다.


### 3.2.3 오프셋과 메시지 순서
각 파티션마다 메시지가 저장되는 위치를 오프셋(offset), 오프셋은 파티션 내에서 유일하고 순차적으로 증가하는 
숫자(64비트 정수) 형태로 되어있다.(Unique id)로 생각하자
> ? 토픽의 메시지 순서는 어떻게 보장하지 ?<br>
> 파티션과 파티션 사이에서는 순서를 보장하지 않는다.<br>
> 참고 자료: https://twowinsh87.github.io/etc/2018/08/11/etc-kafka-13/


## 3.3 카프카의 고가용성을 주도하는 리플리케이션
리플리케이션 팩터(Replication Factor) 기본값 1, 토픽별로 설정가능하지만, 
브로커로  default 값을 설정해 설정안되어 있는 토픽은 default 값으로 설정하도록 할 수 있다.<br>

> ? 리플리케이션 팩터 적용가능한 value 최대값은 ? 브로커(카프카)의 갯수

토픽별로 리플리케이션 되는것이 아닌 패키지별로 리플리케이션 된다.

> topic으로 통하는 모든 데이터의 read/write는 오직 leader를 통해서 이루어진다.
> follower는 주기적으로 leader를 복사해간다.

ISR(In Sync Replica) 그룹에 속하지 않은 follower들은 leader 선정과정에서 제외된다. 
leader와 동기화처리가 늦은 follower들은 ISR 그룹에 속하지 않도록 하여 데이터의 신뢰성을 높이는 전략



## 3.4 카프카에서 사용하는 주키퍼 지노드의 역활
주키퍼의 지노드는 카프카 클러스터 관리를 위한 정보 저장소 역활을 하는 것으로 보인다.

> 0.8.x 이하 버전의 경우엔 주키퍼 지노드에서 "커슈머 offset" 정보를 가지고 있었으나 
> 0.9.x 이상부터 카프카의 내부 토픽에 저장한다고 한다. 내부 토픽도 보관주기가 있으며 기본값은 7일로 되어있고 
> `offsets.retention.minutes` 값으로 수정가능하다.